{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Componentes Principais (PCA)\n",
    "\n",
    "Grandes quantidade de variáveis explicativas são cada vez mais comuns, porém quanto maior a quantidade de varáveis, mais díficil é interpretar a solução. A análise de componentes principais (Principal Component Analysis, PCA) é uma técnica para reduzir a dimensionalidade desses conjuntos de dados, aumentando a interpretabilidade e ao mesmo tempo minimizando a perda de informações. Isso é feito criando novas variáveis não correlacionadas presevam  o máximo de variabilidade possível. Preservar o máximo de variabilidade possível, se traduz em encontrar novas variáveis que são funções lineares daquelas no conjunto de dados original.\n",
    "\n",
    "\n",
    "Em regressão linear, geralmente determinamos a linha de melhor ajuste ao conjunto de dados, mas aqui no PCA, determinamos várias linhas ortogonais entre si no espaço n-dimensional, de melhor ajuste ao conjunto de dados. Ortogonal significa que essas linhas estão em ângulo reto entre si. O número de dimensões será o mesmo que o número de variáveis. Por exemplo, um conjunto de dados com 3 variáveis terá espaço tridimensional. \n",
    "\n",
    "A análise de componentes principais é essencialmente apenas uma transformação de coordenadas. Considerando dados bidimencionais, por exemplo, os dados originais são plotados em um eixo X' e um eixo Y'. O método de PCA procura girar esses dois eixos para que o novo eixo X fique ao longo da direção da variação máxima nos dados. Como a técnica exige que os eixos sejam perpendiculares, em duas dimensões, a escolha de X' determinará Y'. Você obtém os dados transformados lendo os valores x e y deste novo conjunto de eixos, X 'e Y'. Para mais de duas dimensões, o primeiro eixo está na direção da maior da variação; o segundo, na direção da segunda maior variação; e assim por diante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimento para uma análise de componentes principais\n",
    "\n",
    "Considere a matriz $X_{n \\times p}$ composta por observações de $p$ características de $n$ indivíduos de uma população. As características observadas são representadas pelas variáveis $X_1, X_2, X_3, \\cdots, X_p$.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  X = \\left [ \\begin{array}{ccccc}\n",
    "x_{11} & x_{12} & x_{13} & \\cdots & x_{1p}\\\\ \n",
    "x_{21} & x_{22} & x_{23} & \\cdots & x_{2p}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} & \\cdots & x_{np}\\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Passo 1**:\n",
    "Para dados em que as variáveis $X_i$ então em escalas diferentes (por exemplo $X_1$ representa o valor de um carro e $X_2$ o consumo de gasolina), é necessário paronizar os dados. Isso porque os componentes são influenciados pela escala das variáveis, justamente porque as matrizes de covariâncias, $\\Sigma$ ou $\\hat{\\Sigma} = S$,são sensíveis à escala de um par de variáveis. Considere  $\\bar{x_j}$  a média da variável $X_j$; $s(X_j)$ o desvio padrão de $X_j$;  $i = 1, 2,3,4,\\cdots, n$ e  $j = 1, 2,3,4,\\cdots, p$, há duas formas de padronização a serem consideradas: \n",
    "\n",
    "- Média 0 e desvio padrão 1: \n",
    "\n",
    "$$ x'_{ij}= \\frac{x_{ij}-\\bar{X_j}}{s(X_j)} $$ \n",
    "\n",
    "- Média qualquer e desvio padrão 1:\n",
    "\n",
    "$$ x'_{ij}= \\frac{x_{ij}}{s(X_j)} $$ \n",
    "\n",
    "<br>\n",
    "\n",
    "**Passo 2**:\n",
    "Calcular a matriz de **covariância** ou **correlação**. Caso as variáveis estejam em escalas diferentes, é possível calcular a matriz de correlação nos dados originais. Esta possibilidade se deve ao fato de que a matriz de covariâncias das variáveis padronizadas é igual a matriz de correlação das variáveis originais. \n",
    "\n",
    "\\begin{equation}\n",
    "  S = \\left [ \\begin{array}{ccccc}\n",
    "\\hat{Var}(x_1) & \\hat{Cov}(x_1x_2) & \\hat{Cov}(x_1x_3) & \\cdots & \\hat{Cov}(x_1x_p)\\\\ \n",
    "\\hat{Cov}(x_2x_1) &\\hat{Var}(x_2)& \\hat{Cov}(x_2x_3) & \\cdots & \\hat{Cov}(x_2x_p)\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "\\hat{Cov}(x_px_1) & \\hat{Cov}(x_px_2)  & \\hat{Cov}(x_px_3)  & \\cdots & \\hat{Var}(x_p)\\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "  R = \\left [ \\begin{array}{ccccc}\n",
    "1 & r(x_1x_2) & r(x_1x_3) & \\cdots & r(x_1x_p)\\\\ \n",
    "r(x_2x_1) & 1 & r(x_2x_3) & \\cdots & r(x_2x_p)\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "r(x_px_1) & r(x_px_2)  & r(x_px_3)  & \\cdots & 1\\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "Em que:\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{array}{ccc}\n",
    "\\hat{Var}(x_j) = \\frac{\\sum_{i=1}^{n}(x_{ij}-\\bar{x}_j}{n-1}, & \n",
    "\\hat{Cov}(x_{j1},x_{j2}) = \\frac{\\sum_{i=1}^n(x_{ij1}-\\bar{x_{j1}})(x_{ij2}-\\bar{x_{j2}})}{n-1}, &\n",
    "r(x_{j1},x_{j2}) = \\frac{\\hat{Cov}(x_{j1},x_{j2})}{S_{xj1}S_{xj2}}\n",
    "   \\end{array} \n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "**Passo 3**:\n",
    "As componentes principais são determinadas através da equação característica da matriz S ou R:\n",
    "\n",
    "$$det[R - \\lambda I]= 0 $$\n",
    "\n",
    "Em que $I$ é a matriz identidade de dimensão $p\\times p$. \n",
    "\n",
    "Se R ou S tem posto completo igual a $p$, então $det[R - \\lambda I]= 0$, que pode ser reescrito como $\\mid R - \\lambda I \\mid = 0$, terá $p$ soluções. Lembrando que ter posto completo significa que nenhuma coluna é combinação linear de outra.\n",
    "\n",
    "Considere que $\\lambda_1,\\lambda_2,\\lambda_3, \\cdots, \\lambda_p$ sejam as raízes da equação característica de R ou S, então temos que  $\\lambda_1 > \\lambda_2 > \\lambda_3 > \\cdots, \\lambda_p$. Chamamos $\\lambda_i$ de autovalor. Além disso, para cada autovalor há um autovetor $\\tilde{a}_i$ associado.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tilde{a}_i = \\left [ \\begin{array}{c}\n",
    "a_{i1}\\\\ \n",
    "a_{i2}\\\\ \n",
    "\\vdots \\\\\n",
    "a_{ip} \\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "O cálculo do autovetor $\\tilde{a}_i$, pode ser realizado considerando a seguinte propriedade:\n",
    "\n",
    "$$ R\\tilde{a}_i =  \\lambda_i \\tilde{a}_i $$\n",
    "\n",
    "Este resultado deve ser normalizados:\n",
    "\n",
    "$$ a_i = \\frac{\\tilde{a}_i }{\\mid \\tilde{a}_i  \\mid}$$\n",
    "\n",
    "desta forma  soma dos quadrados dos coeficientes é igual a 1 e são ortogonais entre si. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Passo 4**:\n",
    "O cálculo da i-ésima componente principal é dado por:\n",
    "\n",
    "$$Z_i = a_{i1}X_1 + a_{i2}X_2 + a_{i3}X_3 + \\cdots + a_{ip}X_p $$\n",
    "\n",
    "em que $a_{i1}$ são as componetes do autovetor $a_i$ associado ao autovalor $\\lambda_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriedades de PCA\n",
    "\n",
    "- A variância do componente principal $Z_i$ é igual ao valor do autovalor $\\lambda_i$\n",
    "$$ \\hat{Var}(Z_i) = \\lambda_i $$ \n",
    "\n",
    "\n",
    "\n",
    "- O primeiro componente é o que apresenta maior variância e assim por diante\n",
    "$$  \\hat{Var}(Z_1) >  \\hat{Var}(Z_2) >  \\hat{Var}(Z_3) > \\cdots  > \\hat{Var}(Z_p)$$\n",
    "\n",
    "\n",
    "-  O total de variância das variáveis originais é igual ao somatório dos autovalores que é igual ao total de variância dos componentes principais:\n",
    "\n",
    "$$ \\sum_{i=1}^{p} \\hat{Var}(X_i) = \\sum_{i=1}^{p} \\lambda_i = \\sum_{i=1}^{p} \\hat{Var}(Z_i) $$\n",
    "\n",
    "\n",
    "\n",
    "- Os componentes principais não são correlacionados entre si:\n",
    "$$ \\hat{Cov}(Z_{j1},Z_{j2})=0$$\n",
    "\n",
    "\n",
    "- A contribuição Ci de cada componenete principal $Z_i$ é expressa em porcentagem. É calculada dividindo-se a variância de $Z_i$ pela varância total. Representa a proporção de variância total explicada pelo componenete principal $Z_i$.\n",
    "\n",
    "$$ C_i = \\frac{\\lambda_i}{\\sum_{i=1}^{p} \\lambda_i }$$\n",
    "\n",
    "\n",
    "## Interpretação das componentes\n",
    "\n",
    "Por fim, pode-se verificar o grau de influência que cada variável $X_j$ tem sobre a componente $Z_i$. O grau de influência é dado pela corelação entre cada $X_j$ e o componente $Z_i$ que está sendo interpretado. Por exemplo a correlação entre $X_j$ e $Z_1$ é:\n",
    "\n",
    "$$ \\hat{Cov}(X_{j},Z_{1})= \\lambda_1 \\frac{a_{1j}}{\\sqrt{ \\hat{Var}(X_j)}}$$\n",
    "\n",
    "Para comparar a influência de $X_1, X_2, \\cdots, X_p$ sobre $Z_1$ análisamos o peso de cada variável sobre o componente $Z_1$. O peso de cada variável sobre um determinado componente é dado por:\n",
    "\n",
    "$$w_i=\\frac{a_{1j}}{\\sqrt{ \\hat{Var}(X_j)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Discriminante Linear (LDA)\n",
    "\n",
    "A Análise Discriminante Linear é uma técnica que visa transformar as observações multivariadas, por meio de combinações lineares dessas variáveis, em observações univariadas de tal forma que as variáveis transformadas se apresentassem o mais separadas possível. Portanto, a análise discriminante é uma técnica de análise multivariada utilizada para diferenciar ou discriminar populações e classificar ou alocar indivíduos em populações pré-definidas. \n",
    "\n",
    "Vale ressaltar que esta técnica também pode ser usada apenas para reduzir a dimensionalidade. Por ser usada para modelar diferenças em grupos, ou seja, separar duas ou mais classes, ao ser usada como redutora de dimensionalidade, a transformação linear garante que no novo espaço as classes terão máxima separabilidade.\n",
    "\n",
    "\n",
    "## Procedimento de análise\n",
    "\n",
    "\n",
    "Suponha que se tenha uma base de dados com $p$ varáveis explicativas e variável resposta categórica $Y$ com duas possíveis classes. A análise LDA busca obter um escalar $z$ através da projeção sas amostras $X$ em uma reta $z = w^{𝑇}𝑥$, de forma que, de todas as  possíveis retas, gostaríamos de selecionar a que maximiza a separabilidade dos escalares (conforme imagem abaixo). Para encontrar um bom vetor de projeção, precisamos definir uma medida de separação.\n",
    "\n",
    "![](lda.png)\n",
    "\n",
    "Para simplificar, vamos considerar o caso em que $Y$ possui duas classes($a$ e $b$), lembrando que a generalização é valida para variáveis resposta com maior quantidades de categorias. No cálculo usaremos o conceito de discriminante linear de Fisher.\n",
    "\n",
    "\n",
    "**Passo 1**:\n",
    "Considere que o conjunto de dados possa ser separado em dois grupos: grupo A em que observações em que $Y=a$ e grupo B em que observações em que $Y=b$.\n",
    "\n",
    "**Passo 2**:\n",
    "Cálculo da média das variáveis $X_i$ para cada grupo.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{cc}\n",
    "\\hat{\\mu}_a = \\bar{x}_a = \n",
    " \\left [ \\begin{array}{c}\n",
    "X_{1 \\mid Y=a}\\\\ \n",
    "X_{2 \\mid Y=a}\\\\ \n",
    "\\vdots \\\\\n",
    "X_{p \\mid Y=a}\\\\ \n",
    "   \\end{array} \\right ],  & \n",
    " \\hat{\\mu}_b = \\bar{x}_b = \n",
    " \\left [ \\begin{array}{c}\n",
    "X_{1 \\mid Y=b}\\\\ \n",
    "X_{2 \\mid Y=b}\\\\ \n",
    "\\vdots \\\\\n",
    "X_{p \\mid Y=b}\\\\ \n",
    "   \\end{array} \\right ] \n",
    "  \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Passo 3**:\n",
    "Para maximizar a diferença entre as médias, calculamos a medida da dispersão dentro de cada grupo $k \\in \\{a,b\\}$, isto é a variância:\n",
    "\n",
    "$$S_{k} = \\frac{\\sum_{i=1}^{n_k}(x_{ij} -\\bar{X}_j)^2 }{n_k-1} $$\n",
    "\n",
    "em que $n_k$ é a quantidade de onbervações tal que $Y=k \\mid k \\in  \\{a,b\\}$\n",
    "\n",
    "\n",
    "**Passo 4**: \n",
    "Considere dispersão total dentro das classes:\n",
    "\n",
    "$$S = \\frac{(n_a-1)S_a + (n_b-1)S_b }{n_a+n_b-2} $$\n",
    "\n",
    "\n",
    "**Passo 5**:\n",
    "A transformação linear que maximiza a separação entre $a$ e $b$ é dada por:\n",
    "\n",
    "$$ Z = (\\bar{x}_a - \\bar{x}_b)^{T}S^{-1}$$\n",
    "\n",
    "Note que D é um vetor de dimensão $p\\times1$.\n",
    "\n",
    "**Passo 6**:\n",
    "Ao longo da reta $Z$, o ponto médio entre as médias dos dois grupos é dado por:\n",
    "\n",
    "$$ m= \\frac{1}{2}(\\bar{x}_a - \\bar{x}_b)^{T}S^{-1} (\\bar{x}_a + \\bar{x}_b) $$\n",
    "\n",
    "Se o objetiivo for classificar uma observação $x_0$, então se ao aplicar os valores de $x_0$ na equação do passo 5 e obter $Z_0 \\geq m $, então a observação pertence ao grupo $a$, caso contrário pertence ao grupo $b$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
