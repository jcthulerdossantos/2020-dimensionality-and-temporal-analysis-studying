{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise de Componentes Principais (PCA)\n",
    "\n",
    "Grandes quantidade de vari√°veis explicativas s√£o cada vez mais comuns, por√©m quanto maior a quantidade de var√°veis, mais d√≠ficil √© interpretar a solu√ß√£o. A an√°lise de componentes principais (Principal Component Analysis, PCA) √© uma t√©cnica para reduzir a dimensionalidade desses conjuntos de dados, aumentando a interpretabilidade e ao mesmo tempo minimizando a perda de informa√ß√µes. Isso √© feito criando novas vari√°veis n√£o correlacionadas presevam  o m√°ximo de variabilidade poss√≠vel. Preservar o m√°ximo de variabilidade poss√≠vel, se traduz em encontrar novas vari√°veis que s√£o fun√ß√µes lineares daquelas no conjunto de dados original.\n",
    "\n",
    "\n",
    "Em regress√£o linear, geralmente determinamos a linha de melhor ajuste ao conjunto de dados, mas aqui no PCA, determinamos v√°rias linhas ortogonais entre si no espa√ßo n-dimensional, de melhor ajuste ao conjunto de dados. Ortogonal significa que essas linhas est√£o em √¢ngulo reto entre si. O n√∫mero de dimens√µes ser√° o mesmo que o n√∫mero de vari√°veis. Por exemplo, um conjunto de dados com 3 vari√°veis ter√° espa√ßo tridimensional. \n",
    "\n",
    "A an√°lise de componentes principais √© essencialmente apenas uma transforma√ß√£o de coordenadas. Considerando dados bidimencionais, por exemplo, os dados originais s√£o plotados em um eixo X' e um eixo Y'. O m√©todo de PCA procura girar esses dois eixos para que o novo eixo X fique ao longo da dire√ß√£o da varia√ß√£o m√°xima nos dados. Como a t√©cnica exige que os eixos sejam perpendiculares, em duas dimens√µes, a escolha de X' determinar√° Y'. Voc√™ obt√©m os dados transformados lendo os valores x e y deste novo conjunto de eixos, X 'e Y'. Para mais de duas dimens√µes, o primeiro eixo est√° na dire√ß√£o da maior da varia√ß√£o; o segundo, na dire√ß√£o da segunda maior varia√ß√£o; e assim por diante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimento para uma an√°lise de componentes principais\n",
    "\n",
    "Considere a matriz $X_{n \\times p}$ composta por observa√ß√µes de $p$ caracter√≠sticas de $n$ indiv√≠duos de uma popula√ß√£o. As caracter√≠sticas observadas s√£o representadas pelas vari√°veis $X_1, X_2, X_3, \\cdots, X_p$.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  X = \\left [ \\begin{array}{ccccc}\n",
    "x_{11} & x_{12} & x_{13} & \\cdots & x_{1p}\\\\ \n",
    "x_{21} & x_{22} & x_{23} & \\cdots & x_{2p}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} & \\cdots & x_{np}\\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Passo 1**:\n",
    "Para dados em que as vari√°veis $X_i$ ent√£o em escalas diferentes (por exemplo $X_1$ representa o valor de um carro e $X_2$ o consumo de gasolina), √© necess√°rio paronizar os dados. Isso porque os componentes s√£o influenciados pela escala das vari√°veis, justamente porque as matrizes de covari√¢ncias, $\\Sigma$ ou $\\hat{\\Sigma} = S$,s√£o sens√≠veis √† escala de um par de vari√°veis. Considere  $\\bar{x_j}$  a m√©dia da vari√°vel $X_j$; $s(X_j)$ o desvio padr√£o de $X_j$;  $i = 1, 2,3,4,\\cdots, n$ e  $j = 1, 2,3,4,\\cdots, p$, h√° duas formas de padroniza√ß√£o a serem consideradas: \n",
    "\n",
    "- M√©dia 0 e desvio padr√£o 1: \n",
    "\n",
    "$$ x'_{ij}= \\frac{x_{ij}-\\bar{X_j}}{s(X_j)} $$ \n",
    "\n",
    "- M√©dia qualquer e desvio padr√£o 1:\n",
    "\n",
    "$$ x'_{ij}= \\frac{x_{ij}}{s(X_j)} $$ \n",
    "\n",
    "<br>\n",
    "\n",
    "**Passo 2**:\n",
    "Calcular a matriz de **covari√¢ncia** ou **correla√ß√£o**. Caso as vari√°veis estejam em escalas diferentes, √© poss√≠vel calcular a matriz de correla√ß√£o nos dados originais. Esta possibilidade se deve ao fato de que a matriz de covari√¢ncias das vari√°veis padronizadas √© igual a matriz de correla√ß√£o das vari√°veis originais. \n",
    "\n",
    "\\begin{equation}\n",
    "  S = \\left [ \\begin{array}{ccccc}\n",
    "\\hat{Var}(x_1) & \\hat{Cov}(x_1x_2) & \\hat{Cov}(x_1x_3) & \\cdots & \\hat{Cov}(x_1x_p)\\\\ \n",
    "\\hat{Cov}(x_2x_1) &\\hat{Var}(x_2)& \\hat{Cov}(x_2x_3) & \\cdots & \\hat{Cov}(x_2x_p)\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "\\hat{Cov}(x_px_1) & \\hat{Cov}(x_px_2)  & \\hat{Cov}(x_px_3)  & \\cdots & \\hat{Var}(x_p)\\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "  R = \\left [ \\begin{array}{ccccc}\n",
    "1 & r(x_1x_2) & r(x_1x_3) & \\cdots & r(x_1x_p)\\\\ \n",
    "r(x_2x_1) & 1 & r(x_2x_3) & \\cdots & r(x_2x_p)\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots  \\\\\n",
    "r(x_px_1) & r(x_px_2)  & r(x_px_3)  & \\cdots & 1\\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "Em que:\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{array}{ccc}\n",
    "\\hat{Var}(x_j) = \\frac{\\sum_{i=1}^{n}(x_{ij}-\\bar{x}_j}{n-1}, & \n",
    "\\hat{Cov}(x_{j1},x_{j2}) = \\frac{\\sum_{i=1}^n(x_{ij1}-\\bar{x_{j1}})(x_{ij2}-\\bar{x_{j2}})}{n-1}, &\n",
    "r(x_{j1},x_{j2}) = \\frac{\\hat{Cov}(x_{j1},x_{j2})}{S_{xj1}S_{xj2}}\n",
    "   \\end{array} \n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "**Passo 3**:\n",
    "As componentes principais s√£o determinadas atrav√©s da equa√ß√£o caracter√≠stica da matriz S ou R:\n",
    "\n",
    "$$det[R - \\lambda I]= 0 $$\n",
    "\n",
    "Em que $I$ √© a matriz identidade de dimens√£o $p\\times p$. \n",
    "\n",
    "Se R ou S tem posto completo igual a $p$, ent√£o $det[R - \\lambda I]= 0$, que pode ser reescrito como $\\mid R - \\lambda I \\mid = 0$, ter√° $p$ solu√ß√µes. Lembrando que ter posto completo significa que nenhuma coluna √© combina√ß√£o linear de outra.\n",
    "\n",
    "Considere que $\\lambda_1,\\lambda_2,\\lambda_3, \\cdots, \\lambda_p$ sejam as ra√≠zes da equa√ß√£o caracter√≠stica de R ou S, ent√£o temos que  $\\lambda_1 > \\lambda_2 > \\lambda_3 > \\cdots, \\lambda_p$. Chamamos $\\lambda_i$ de autovalor. Al√©m disso, para cada autovalor h√° um autovetor $\\tilde{a}_i$ associado.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tilde{a}_i = \\left [ \\begin{array}{c}\n",
    "a_{i1}\\\\ \n",
    "a_{i2}\\\\ \n",
    "\\vdots \\\\\n",
    "a_{ip} \\\\ \n",
    "   \\end{array} \\right ] \n",
    "\\end{equation}\n",
    "\n",
    "O c√°lculo do autovetor $\\tilde{a}_i$, pode ser realizado considerando a seguinte propriedade:\n",
    "\n",
    "$$ R\\tilde{a}_i =  \\lambda_i \\tilde{a}_i $$\n",
    "\n",
    "Este resultado deve ser normalizados:\n",
    "\n",
    "$$ a_i = \\frac{\\tilde{a}_i }{\\mid \\tilde{a}_i  \\mid}$$\n",
    "\n",
    "desta forma  soma dos quadrados dos coeficientes √© igual a 1 e s√£o ortogonais entre si. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Passo 4**:\n",
    "O c√°lculo da i-√©sima componente principal √© dado por:\n",
    "\n",
    "$$Z_i = a_{i1}X_1 + a_{i2}X_2 + a_{i3}X_3 + \\cdots + a_{ip}X_p $$\n",
    "\n",
    "em que $a_{i1}$ s√£o as componetes do autovetor $a_i$ associado ao autovalor $\\lambda_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriedades de PCA\n",
    "\n",
    "- A vari√¢ncia do componente principal $Z_i$ √© igual ao valor do autovalor $\\lambda_i$\n",
    "$$ \\hat{Var}(Z_i) = \\lambda_i $$ \n",
    "\n",
    "\n",
    "\n",
    "- O primeiro componente √© o que apresenta maior vari√¢ncia e assim por diante\n",
    "$$  \\hat{Var}(Z_1) >  \\hat{Var}(Z_2) >  \\hat{Var}(Z_3) > \\cdots  > \\hat{Var}(Z_p)$$\n",
    "\n",
    "\n",
    "-  O total de vari√¢ncia das vari√°veis originais √© igual ao somat√≥rio dos autovalores que √© igual ao total de vari√¢ncia dos componentes principais:\n",
    "\n",
    "$$ \\sum_{i=1}^{p} \\hat{Var}(X_i) = \\sum_{i=1}^{p} \\lambda_i = \\sum_{i=1}^{p} \\hat{Var}(Z_i) $$\n",
    "\n",
    "\n",
    "\n",
    "- Os componentes principais n√£o s√£o correlacionados entre si:\n",
    "$$ \\hat{Cov}(Z_{j1},Z_{j2})=0$$\n",
    "\n",
    "\n",
    "- A contribui√ß√£o Ci de cada componenete principal $Z_i$ √© expressa em porcentagem. √â calculada dividindo-se a vari√¢ncia de $Z_i$ pela var√¢ncia total. Representa a propor√ß√£o de vari√¢ncia total explicada pelo componenete principal $Z_i$.\n",
    "\n",
    "$$ C_i = \\frac{\\lambda_i}{\\sum_{i=1}^{p} \\lambda_i }$$\n",
    "\n",
    "\n",
    "## Interpreta√ß√£o das componentes\n",
    "\n",
    "Por fim, pode-se verificar o grau de influ√™ncia que cada vari√°vel $X_j$ tem sobre a componente $Z_i$. O grau de influ√™ncia √© dado pela corela√ß√£o entre cada $X_j$ e o componente $Z_i$ que est√° sendo interpretado. Por exemplo a correla√ß√£o entre $X_j$ e $Z_1$ √©:\n",
    "\n",
    "$$ \\hat{Cov}(X_{j},Z_{1})= \\lambda_1 \\frac{a_{1j}}{\\sqrt{ \\hat{Var}(X_j)}}$$\n",
    "\n",
    "Para comparar a influ√™ncia de $X_1, X_2, \\cdots, X_p$ sobre $Z_1$ an√°lisamos o peso de cada vari√°vel sobre o componente $Z_1$. O peso de cada vari√°vel sobre um determinado componente √© dado por:\n",
    "\n",
    "$$w_i=\\frac{a_{1j}}{\\sqrt{ \\hat{Var}(X_j)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise Discriminante Linear (LDA)\n",
    "\n",
    "A An√°lise Discriminante Linear √© uma t√©cnica que visa transformar as observa√ß√µes multivariadas, por meio de combina√ß√µes lineares dessas vari√°veis, em observa√ß√µes univariadas de tal forma que as vari√°veis transformadas se apresentassem o mais separadas poss√≠vel. Portanto, a an√°lise discriminante √© uma t√©cnica de an√°lise multivariada utilizada para diferenciar ou discriminar popula√ß√µes e classificar ou alocar indiv√≠duos em popula√ß√µes pr√©-definidas. \n",
    "\n",
    "Vale ressaltar que esta t√©cnica tamb√©m pode ser usada apenas para reduzir a dimensionalidade. Por ser usada para modelar diferen√ßas em grupos, ou seja, separar duas ou mais classes, ao ser usada como redutora de dimensionalidade, a transforma√ß√£o linear garante que no novo espa√ßo as classes ter√£o m√°xima separabilidade.\n",
    "\n",
    "\n",
    "## Procedimento de an√°lise\n",
    "\n",
    "\n",
    "Suponha que se tenha uma base de dados com $p$ var√°veis explicativas e vari√°vel resposta categ√≥rica $Y$ com duas poss√≠veis classes. A an√°lise LDA busca obter um escalar $z$ atrav√©s da proje√ß√£o sas amostras $X$ em uma reta $z = w^{ùëá}ùë•$, de forma que, de todas as  poss√≠veis retas, gostar√≠amos de selecionar a que maximiza a separabilidade dos escalares (conforme imagem abaixo). Para encontrar um bom vetor de proje√ß√£o, precisamos definir uma medida de separa√ß√£o.\n",
    "\n",
    "![](lda.png)\n",
    "\n",
    "Para simplificar, vamos considerar o caso em que $Y$ possui duas classes($a$ e $b$), lembrando que a generaliza√ß√£o √© valida para vari√°veis resposta com maior quantidades de categorias. No c√°lculo usaremos o conceito de discriminante linear de Fisher.\n",
    "\n",
    "\n",
    "**Passo 1**:\n",
    "Considere que o conjunto de dados possa ser separado em dois grupos: grupo A em que observa√ß√µes em que $Y=a$ e grupo B em que observa√ß√µes em que $Y=b$.\n",
    "\n",
    "**Passo 2**:\n",
    "C√°lculo da m√©dia das vari√°veis $X_i$ para cada grupo.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{cc}\n",
    "\\hat{\\mu}_a = \\bar{x}_a = \n",
    " \\left [ \\begin{array}{c}\n",
    "X_{1 \\mid Y=a}\\\\ \n",
    "X_{2 \\mid Y=a}\\\\ \n",
    "\\vdots \\\\\n",
    "X_{p \\mid Y=a}\\\\ \n",
    "   \\end{array} \\right ],  & \n",
    " \\hat{\\mu}_b = \\bar{x}_b = \n",
    " \\left [ \\begin{array}{c}\n",
    "X_{1 \\mid Y=b}\\\\ \n",
    "X_{2 \\mid Y=b}\\\\ \n",
    "\\vdots \\\\\n",
    "X_{p \\mid Y=b}\\\\ \n",
    "   \\end{array} \\right ] \n",
    "  \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Passo 3**:\n",
    "Para maximizar a diferen√ßa entre as m√©dias, calculamos a medida da dispers√£o dentro de cada grupo $k \\in \\{a,b\\}$, isto √© a vari√¢ncia:\n",
    "\n",
    "$$S_{k} = \\frac{\\sum_{i=1}^{n_k}(x_{ij} -\\bar{X}_j)^2 }{n_k-1} $$\n",
    "\n",
    "em que $n_k$ √© a quantidade de onberva√ß√µes tal que $Y=k \\mid k \\in  \\{a,b\\}$\n",
    "\n",
    "\n",
    "**Passo 4**: \n",
    "Considere dispers√£o total dentro das classes:\n",
    "\n",
    "$$S = \\frac{(n_a-1)S_a + (n_b-1)S_b }{n_a+n_b-2} $$\n",
    "\n",
    "\n",
    "**Passo 5**:\n",
    "A transforma√ß√£o linear que maximiza a separa√ß√£o entre $a$ e $b$ √© dada por:\n",
    "\n",
    "$$ Z = (\\bar{x}_a - \\bar{x}_b)^{T}S^{-1}$$\n",
    "\n",
    "Note que D √© um vetor de dimens√£o $p\\times1$.\n",
    "\n",
    "**Passo 6**:\n",
    "Ao longo da reta $Z$, o ponto m√©dio entre as m√©dias dos dois grupos √© dado por:\n",
    "\n",
    "$$ m= \\frac{1}{2}(\\bar{x}_a - \\bar{x}_b)^{T}S^{-1} (\\bar{x}_a + \\bar{x}_b) $$\n",
    "\n",
    "Se o objetiivo for classificar uma observa√ß√£o $x_0$, ent√£o se ao aplicar os valores de $x_0$ na equa√ß√£o do passo 5 e obter $Z_0 \\geq m $, ent√£o a observa√ß√£o pertence ao grupo $a$, caso contr√°rio pertence ao grupo $b$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
